{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPdsZXxICYMZpWZ+VL6h/H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ToLYmFIvAVDg","executionInfo":{"status":"ok","timestamp":1679399443792,"user_tz":-540,"elapsed":20472,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"d9c383cd-56d7-43b9-deb0-762545b5793b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scrapy\n","  Downloading Scrapy-2.8.0-py2.py3-none-any.whl (272 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.9/272.9 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyOpenSSL>=21.0.0\n","  Downloading pyOpenSSL-23.0.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Twisted>=18.9.0\n","  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting w3lib>=1.17.0\n","  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n","Collecting PyDispatcher>=2.0.5\n","  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n","Collecting queuelib>=1.4.2\n","  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n","Collecting protego>=0.1.15\n","  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n","Collecting tldextract\n","  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting parsel>=1.5.0\n","  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\n","Collecting service-identity>=18.1.0\n","  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n","Collecting itemadapter>=0.1.0\n","  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n","Collecting zope.interface>=5.1.0\n","  Downloading zope.interface-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.1/246.1 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cryptography>=3.4.6\n","  Downloading cryptography-39.0.2-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from scrapy) (63.4.3)\n","Requirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from scrapy) (4.9.2)\n","Collecting cssselect>=0.9.1\n","  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n","Collecting itemloaders>=1.0.1\n","  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from scrapy) (23.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=3.4.6->scrapy) (1.15.1)\n","Collecting jmespath>=0.9.5\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from protego>=0.1.15->scrapy) (1.15.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.9/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from service-identity>=18.1.0->scrapy) (22.2.0)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.9/dist-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n","Collecting constantly>=15.1\n","  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n","Collecting hyperlink>=17.1.1\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.9/dist-packages (from Twisted>=18.9.0->scrapy) (4.5.0)\n","Collecting Automat>=0.8.0\n","  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n","Collecting incremental>=21.3.0\n","  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n","Collecting requests-file>=1.4\n","  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.9/dist-packages (from tldextract->scrapy) (3.10.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.9/dist-packages (from tldextract->scrapy) (3.4)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from tldextract->scrapy) (2.27.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=3.4.6->scrapy) (2.21)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.12.7)\n","Installing collected packages: PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, Automat, Twisted, requests-file, parsel, cryptography, tldextract, service-identity, pyOpenSSL, itemloaders, scrapy\n","Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 constantly-15.1.0 cryptography-39.0.2 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 jmespath-1.0.1 parsel-1.7.0 protego-0.2.1 pyOpenSSL-23.0.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.8.0 service-identity-21.1.0 tldextract-3.4.0 w3lib-2.1.1 zope.interface-6.0\n"]}],"source":["!pip install scrapy"]},{"cell_type":"code","source":["!scrapy startproject news_crawler\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pUn-NFymAYW5","executionInfo":{"status":"ok","timestamp":1679399445885,"user_tz":-540,"elapsed":2096,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"c484973b-b768-4e26-f18f-30e4149bb67f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["New Scrapy project 'news_crawler', using template directory '/usr/local/lib/python3.9/dist-packages/scrapy/templates/project', created in:\n","    /content/news_crawler\n","\n","You can start your first spider with:\n","    cd news_crawler\n","    scrapy genspider example example.com\n"]}]},{"cell_type":"code","source":["cd ./news_crawler/news_crawler/spiders/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r54AoEBPAZKU","executionInfo":{"status":"ok","timestamp":1679399517526,"user_tz":-540,"elapsed":2,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"7528d1aa-4a2c-4039-c05d-500292d989d9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/news_crawler/news_crawler/spiders\n"]}]},{"cell_type":"code","source":["pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Tgqf4vB5AvkV","executionInfo":{"status":"ok","timestamp":1679399520796,"user_tz":-540,"elapsed":4,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"24e60802-38e2-4021-9573-f4c102a6d1af"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/news_crawler/news_crawler/spiders'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcCYFs3WAwTh","executionInfo":{"status":"ok","timestamp":1679399523023,"user_tz":-540,"elapsed":4,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"b12b3a51-6a0c-4e3d-fb12-c0a43f1b55dc"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["__init__.py\n"]}]},{"cell_type":"code","source":["!scrapy genspider daum_news_spider news.daum.net\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t87gcroaAwwk","executionInfo":{"status":"ok","timestamp":1679399530159,"user_tz":-540,"elapsed":1398,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"7a1624a9-9e01-4c2e-b2be-f4f93d81cbfc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Created spider 'daum_news_spider' using template 'basic' in module:\n","  news_crawler.spiders.daum_news_spider\n"]}]},{"cell_type":"code","source":["!scrapy crawl daum_news_spider\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ydlNrG9RAyY3","executionInfo":{"status":"ok","timestamp":1679400140764,"user_tz":-540,"elapsed":1124,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"ae59d903-5c69-41f4-d586-664f76571498"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-21 12:02:19 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: news_crawler)\n","2023-03-21 12:02:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.9.16 (main, Dec  7 2022, 01:11:51) - [GCC 9.4.0], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform Linux-5.10.147+-x86_64-with-glibc2.31\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/spiderloader.py\", line 77, in load\n","    return self._spiders[spider_name]\n","KeyError: 'daum_news_spider'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/scrapy\", line 8, in <module>\n","    sys.exit(execute())\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 158, in execute\n","    _run_print_help(parser, _run_command, cmd, args, opts)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 111, in _run_print_help\n","    func(*a, **kw)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 166, in _run_command\n","    cmd.run(args, opts)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/commands/crawl.py\", line 24, in run\n","    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 232, in crawl\n","    crawler = self.create_crawler(crawler_or_spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 266, in create_crawler\n","    return self._create_crawler(crawler_or_spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 346, in _create_crawler\n","    spidercls = self.spider_loader.load(spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/spiderloader.py\", line 79, in load\n","    raise KeyError(f\"Spider not found: {spider_name}\")\n","KeyError: 'Spider not found: daum_news_spider'\n"]}]},{"cell_type":"code","source":["pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"BfSuwL0EDAQU","executionInfo":{"status":"ok","timestamp":1679400281185,"user_tz":-540,"elapsed":5,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"cb59baa9-b738-40db-87d3-bb0ad98edc0c"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/news_crawler/news_crawler/spiders'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X78LEFc5Dp04","executionInfo":{"status":"ok","timestamp":1679400537358,"user_tz":-540,"elapsed":2108,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"eaa24e63-cb4f-4269-f1e1-407a6bae86e8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-21 12:08:56 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: news_crawler)\n","2023-03-21 12:08:56 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.9.16 (main, Dec  7 2022, 01:11:51) - [GCC 9.4.0], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform Linux-5.10.147+-x86_64-with-glibc2.31\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/spiderloader.py\", line 77, in load\n","    return self._spiders[spider_name]\n","KeyError: 'daum_news_spider'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/scrapy\", line 8, in <module>\n","    sys.exit(execute())\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 158, in execute\n","    _run_print_help(parser, _run_command, cmd, args, opts)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 111, in _run_print_help\n","    func(*a, **kw)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 166, in _run_command\n","    cmd.run(args, opts)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/commands/crawl.py\", line 24, in run\n","    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 232, in crawl\n","    crawler = self.create_crawler(crawler_or_spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 266, in create_crawler\n","    return self._create_crawler(crawler_or_spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 346, in _create_crawler\n","    spidercls = self.spider_loader.load(spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/spiderloader.py\", line 79, in load\n","    raise KeyError(f\"Spider not found: {spider_name}\")\n","KeyError: 'Spider not found: daum_news_spider'\n"]}]},{"cell_type":"code","source":["cd .."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FrkJ86ugEVmc","executionInfo":{"status":"ok","timestamp":1679400573663,"user_tz":-540,"elapsed":2,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"acdb0b8c-9bb6-47e1-cf21-6515817b5222"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/news_crawler/news_crawler\n"]}]},{"cell_type":"code","source":["pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"7yroi-hMExLj","executionInfo":{"status":"ok","timestamp":1679400575332,"user_tz":-540,"elapsed":4,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"aefaff8a-2765-4ad3-ef28-43e1ba75997b"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/news_crawler/news_crawler'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhU4q8EnExmT","executionInfo":{"status":"ok","timestamp":1679400586001,"user_tz":-540,"elapsed":1804,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"f471e794-f439-4e8a-d813-25ca8cea9798"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-21 12:09:44 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: news_crawler)\n","2023-03-21 12:09:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.9.16 (main, Dec  7 2022, 01:11:51) - [GCC 9.4.0], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform Linux-5.10.147+-x86_64-with-glibc2.31\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/spiderloader.py\", line 77, in load\n","    return self._spiders[spider_name]\n","KeyError: 'daum_news_spider'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/scrapy\", line 8, in <module>\n","    sys.exit(execute())\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 158, in execute\n","    _run_print_help(parser, _run_command, cmd, args, opts)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 111, in _run_print_help\n","    func(*a, **kw)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 166, in _run_command\n","    cmd.run(args, opts)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/commands/crawl.py\", line 24, in run\n","    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 232, in crawl\n","    crawler = self.create_crawler(crawler_or_spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 266, in create_crawler\n","    return self._create_crawler(crawler_or_spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 346, in _create_crawler\n","    spidercls = self.spider_loader.load(spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/spiderloader.py\", line 79, in load\n","    raise KeyError(f\"Spider not found: {spider_name}\")\n","KeyError: 'Spider not found: daum_news_spider'\n"]}]},{"cell_type":"code","source":["cd news_crawler/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZUaPBAfE0DW","executionInfo":{"status":"ok","timestamp":1679400654459,"user_tz":-540,"elapsed":1,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"518fa33f-d1b2-49b2-9cdd-92adba868763"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/news_crawler/news_crawler\n"]}]},{"cell_type":"code","source":["!scrapy crawl daum_news_spider"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vV5yD49dE3yT","executionInfo":{"status":"ok","timestamp":1679400658328,"user_tz":-540,"elapsed":1853,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"0ec04542-5eb6-4d3d-a6d7-065310f1ca34"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-21 12:10:57 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: news_crawler)\n","2023-03-21 12:10:57 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.9.16 (main, Dec  7 2022, 01:11:51) - [GCC 9.4.0], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform Linux-5.10.147+-x86_64-with-glibc2.31\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/spiderloader.py\", line 77, in load\n","    return self._spiders[spider_name]\n","KeyError: 'daum_news_spider'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/scrapy\", line 8, in <module>\n","    sys.exit(execute())\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 158, in execute\n","    _run_print_help(parser, _run_command, cmd, args, opts)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 111, in _run_print_help\n","    func(*a, **kw)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/cmdline.py\", line 166, in _run_command\n","    cmd.run(args, opts)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/commands/crawl.py\", line 24, in run\n","    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 232, in crawl\n","    crawler = self.create_crawler(crawler_or_spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 266, in create_crawler\n","    return self._create_crawler(crawler_or_spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/crawler.py\", line 346, in _create_crawler\n","    spidercls = self.spider_loader.load(spidercls)\n","  File \"/usr/local/lib/python3.9/dist-packages/scrapy/spiderloader.py\", line 79, in load\n","    raise KeyError(f\"Spider not found: {spider_name}\")\n","KeyError: 'Spider not found: daum_news_spider'\n"]}]},{"cell_type":"code","source":["cd news_crawler\n","scrapy crawl daum_news_spider\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"mrci92OnE45X","executionInfo":{"status":"error","timestamp":1679400639065,"user_tz":-540,"elapsed":4,"user":{"displayName":"runa cian","userId":"10855183189387375815"}},"outputId":"d2a0787e-8266-46c6-f732-61e4242ca9a2"},"execution_count":20,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-9deefded08e2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    cd news_crawler\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"nr7LdgddFBe6"},"execution_count":null,"outputs":[]}]}